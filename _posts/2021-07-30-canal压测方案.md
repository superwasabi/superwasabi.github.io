---
layout:     post
title:      Canal压测方案
subtitle:   初步方案
date:       2021-07-30
author:     Max
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - Canal
---

最近由于公司canal集群部分节点binlog日志出现同步延迟的情况，故决定对现有canal集群的性能进行压力测试

#### 测试目的

- 找出同步延迟的原因，对canal相关项目进行优化



#### 测试环境

| 类型         | 配置 |
| ------------ | ---- |
| MySQL        |      |
| Canal Server |      |
| Canal Client |      |



#### 测试节点

Bv-master-canal-wx-0



#### canal配置

目前未对canal进行优化，使用的默认配置



#### 参考文档

[https://github.com/alibaba/canal/wiki/Prometheus-QuickStart]()



#### 测试工具

- **Prometheus**：Prometheus是一个开源系统监控和警报工具包，最初是在SoundCloud 上构建的 。自 2012 年成立以来，许多公司和组织都采用了 Prometheus，该项目拥有非常活跃的开发者和用户社区。
- **Grafana**：grafana 是一款采用 go 语言编写的开源应用，主要用于大规模指标数据的可视化展现，是网络架构和应用分析中最流行的时序数据展示工具，目前已经支持绝大部分常用的时序数据库。



#### 部署流程

1. 安装部署对应平台的Prometheus和Grafana

   ```shell
   #通过brew命令安装Prometheus
   brew install prometheus
   #编写配置文件prometheus.yml（示例）
   vim /usr/local/etc/prometheus.yml
   global:
     scrape_interval:     15s
     evaluation_interval: 15s
   
   rule_files:
     # - "first.rules"
     # - "second.rules"
   
   scrape_configs:
     - job_name: prometheus
       static_configs:
         - targets: ['localhost:9090']#端口配置即为canal.properties中的canal.metrics.pull.port
   #后台运行prometheus
   prometheus --config.file=/usr/local/etc/prometheus.yml &
   #访问localhost:9090就可以看到prometheus的监控页面
   ```

   示例配置文件中配置的三个模块：**global**，**rule_files**，和**scrape_configs**：

   global块控制 Prometheus 服务器的全局配置。我们有两种选择。第一个，scrape_interval，控制 Prometheus 抓取目标的频率。您可以为单个目标覆盖此设置。在这种情况下，全局设置是每 15 秒抓取一次。该evaluation_interval选项控制 Prometheus 评估的频率。Prometheus 使用该间隔来创建新的时间序列并生成警报。

   rule_files块指定了我们希望 Prometheus 服务器加载的任何规则的位置。目前我们没有规则。

   最后一个块scrape_configs控制 Prometheus 监控的资源。由于 Prometheus 还将有关自身的数据公开为 HTTP 端点，因此它可以抓取和监控自己的健康状况。在默认配置中，有一个名为 prometheus的作业，prometheus它抓取 自身 服务器公开的时间序列数据。该作业包含一个静态配置的目标，即localhoston port 9090。Prometheus 期望指标可用于路径上的目标/metrics。所以这个默认作业是通过 URL 抓取的：http://localhost:9090/metrics。返回的时间序列数据将体现出Prometheus 服务器的状态和性能。

   ```shell
   #brew命令安装grafana
   brew install grafana
   #运行grafana
   brew services start grafana
   #浏览器访问http://localhost:3000就可以看到Grafana页面,账号admin，密码admin
   ```

2. 配置[prometheus datasource](http://docs.grafana.org/features/datasources/prometheus/#adding-the-data-source-to-grafana).（将Prometheus作为Grafana的数据源）

3. 导入模板([canal/conf/metrics/Canal_instances_tmpl.json](https://raw.githubusercontent.com/alibaba/canal/master/deployer/src/main/resources/metrics/Canal_instances_tmpl.json))，参考[这里](http://docs.grafana.org/reference/export_import/#importing-a-dashboard)。

   tips：在grafana的控制面板导入官方提供的canal检测指标的json文件，可自动生成监控的dashboard
   
4. 修改代码参数，运行测试代码，批量操作Insert/Update/Delete (导入业务)

   各个阶段测试代码：

   1. [FetcherPerformanceTest.java](https://github.com/alibaba/canal/blob/master/dbsync/src/test/java/com/taobao/tddl/dbsync/FetcherPerformanceTest.java)（binlog接收）
   2. [MysqlBinlogEventPerformanceTest.java](https://github.com/alibaba/canal/blob/master/parse/src/test/java/com/alibaba/otter/canal/parse/MysqlBinlogEventPerformanceTest.java)（binlog event解析）
   3. [MysqlBinlogParsePerformanceTest.java](https://github.com/alibaba/canal/blob/master/parse/src/test/java/com/alibaba/otter/canal/parse/MysqlBinlogParsePerformanceTest.java)（insert/update/delete解析）
   4. [MysqlBinlogDumpPerformanceTest.java](https://github.com/alibaba/canal/blob/master/parse/src/test/java/com/alibaba/otter/canal/parse/MysqlBinlogDumpPerformanceTest.java)（生成CanalEntry ，存储到memory store）
   5. [SimpleCanalClientPermanceTest.java](https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/SimpleCanalClientPermanceTest.java)（client接收）

   # 



#### 压测指标记录

| 是否优化 | 数据规模 | dump进程堵塞 | sink进程堵塞 | eventStore存储 | Cpu占比 | 内存占用 | TPS（吞吐量） | 数据同步延时 | 消息中间件 |
| -------- | -------- | ------------ | ------------ | -------------- | ------- | -------- | ------------- | ------------ | ---------- |
| 未优化   |          |              |              |                |         |          |               |              |            |
| 优化后   |          |              |              |                |         |          |               |              |            |



#### canal监控原始指标

| 指标                                  | 说明                                                         | 单位 | 精度 |
| ------------------------------------- | ------------------------------------------------------------ | ---- | ---- |
| canal_instance_transactions           | instance接收transactions计数                                 | -    | -    |
| canal_instance                        | instance基本信息                                             | -    | -    |
| canal_instance_subscriptions          | instance订阅数量                                             | -    | -    |
| canal_instance_publish_blocking_time  | instance dump线程提交到异步解析队列过程中的阻塞时间(仅parallel解析模式) | ms   | ns   |
| canal_instance_received_binlog_bytes  | instance接收binlog字节数                                     | byte | -    |
| canal_instance_parser_mode            | instance解析模式(是否开启parallel解析)                       | -    | -    |
| canal_instance_client_packets         | instance client请求次数的计数                                | -    | -    |
| canal_instance_client_bytes           | 向instance client发送数据包字节计数                          | byte | -    |
| canal_instance_client_empty_batches   | 向instance client发送get接口的空结果计数                     | -    | -    |
| canal_instance_client_request_error   | instance client请求失败计数                                  | -    | -    |
| canal_instance_client_request_latency | instance client请求的响应时间概况                            | -    | -    |
| canal_instance_sink_blocking_time     | instance sink线程put数据至store的阻塞时间                    | ms   | ns   |
| canal_instance_store_produce_seq      | instance store接收到的events sequence number                 | -    | -    |
| canal_instance_store_consume_seq      | instance store成功消费的events sequence number               | -    | -    |
| canal_instance_store                  | instance store基本信息                                       | -    | -    |
| canal_instance_store_produce_mem      | instance store接收到的所有events占用内存总量                 | byte | -    |
| canal_instance_store_consume_mem      | instance store成功消费的所有events占用内存总量               | byte | -    |
| canal_instance_put_rows               | store put操作完成的table rows                                | -    | -    |
| canal_instance_get_rows               | client get请求返回的table rows                               | -    | -    |
| canal_instance_ack_rows               | client ack操作释放的table rows                               | -    | -    |
| canal_instance_traffic_delay          | server与MySQL master的延时                                   | ms   | ms   |
| canal_instance_put_delay              | store put操作events的延时                                    | ms   | ms   |
| canal_instance_get_delay              | client get请求返回events的延时                               | ms   | ms   |
| canal_instance_ack_delay              | client ack操作释放events的延时                               | ms   | ms   |

#### 监控展示指标

| 指标                                                         | 简述                                                         | 多指标 |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------ |
| [Basic](https://github.com/alibaba/canal/wiki/Canal-prometheus#状态信息) | Canal instance 基本信息。                                    | 是     |
| [Network bandwith](https://github.com/alibaba/canal/wiki/Canal-prometheus#网络带宽kbs) | 网络带宽。包含inbound(canal server读取binlog的网络带宽)和outbound(canal server返回给canal client的网络带宽) | 是     |
| [Delay](https://github.com/alibaba/canal/wiki/Canal-prometheus#delayseconds) | Canal server与master延时；store 的put, get, ack操作对应的延时。 | 是     |
| [Blocking](https://github.com/alibaba/canal/wiki/Canal-prometheus#blocking) | sink线程blocking占比；dump线程blocking占比(仅parallel mode)。 | 是     |
| [TPS(transaction)](https://github.com/alibaba/canal/wiki/Canal-prometheus#tpsmysql-transaction) | Canal instance 处理binlog的TPS，以MySQL transaction为单位计算。 | 否     |
| [TPS(tableRows)](https://github.com/alibaba/canal/wiki/Canal-prometheus#tpstable-row) | 分别对应store的put, get, ack操作针对数据表变更行的TPS        | 是     |
| [Client requests](https://github.com/alibaba/canal/wiki/Canal-prometheus#client-requests) | Canal client请求server的请求数统计，结果按请求类型分类(比如get/ack/sub/rollback等)。 | 否     |
| [Response time](https://github.com/alibaba/canal/wiki/Canal-prometheus#response-time) | Canal client请求server的响应时间统计。                       | 否     |
| [Empty packets](https://github.com/alibaba/canal/wiki/Canal-prometheus#empty-packets) | Canal client请求server返回空结果的统计。                     | 是     |
| [Store remain events](https://github.com/alibaba/canal/wiki/Canal-prometheus#event-store占用) | Canal instance ringbuffer中堆积的events数量。                | 否     |
| [Store remain mem](https://github.com/alibaba/canal/wiki/Canal-prometheus#event-store-memory占用kb-仅memory-mode) | Canal instance ringbuffer中堆积的events内存使用量。          | 否     |
| [Client QPS](https://github.com/alibaba/canal/wiki/Canal-prometheus#client-qps) | client发送请求的QPS，按GET与CLIENTACK分类统计                | 是     |

#### 监控指标详述与应用场景

##### Blocking

![Image text](https://raw.githubusercontent.com/lcybo/canal/master/images/idle.PNG)

```
clamp_max(rate(canal_instance_sink_blocking_time{destination="example"}[2m]), 1000) / 10
```

**sink线程blocking时间片比例(向store中put events时)。若idle占比很高，则store总体上处于满的状态，client的consume速度低于server的produce速度**

```
clamp_max(rate(canal_instance_publish_blocking_time{destination="example"}[2m]), 1000) / 10
```

**dump线程blocking时间片比例(仅parallel mode, dump线程向disruptor发布event时)。若idle占比较高：**

**1. Sinking blocking ratio也很高，则瓶颈是因为client的consume速度相对较慢。**

**2. Sinking blocking ratio较低，那么server端parser是性能瓶颈，可参考[Performance](https://github.com/alibaba/canal/wiki/Performance)进行tuning.**

------

##### Delay(seconds)

![Image text](https://raw.githubusercontent.com/lcybo/canal/master/images/delay.PNG)

```
canal_instance_traffic_delay{destination="example"} / 1000
```

**Server与MySQL master之间的延时。**

```
canal_instance_put_delay{destination="example"} / 1000
```

**Store put操作时间点的延时。**

```
canal_instance_get_delay{destination="example"} / 1000
```

**Client get操作时间点的延时。**

```
canal_instance_ack_delay{destination="example"} / 1000
```

**Client ack操作时间点的延时。**

**Note: delay的准确度依赖于master与canal server间的ntp同步。当binlog execTime超过canal server当前时间戳，则delay为0.**

------

##### 网络带宽(KB/s)

![Image text](https://github.com/lcybo/canal/raw/master/images/network.PNG?raw=true)

```
rate(canal_instance_received_binlog_bytes{destination="example"}[2m]) / 1024
```

**Dump线程读取binlog所占用带宽。当'Sink线程空闲比'与'Dump线程空闲比'都很低，delay却比较高的情况，请查看binlog接收速率是否符合预期。**

```
rate(canal_instance_client_bytes{destination="example"}[2m]) / 1024
```

**向Instance client发送格式化binlog所占用的带宽。MySQL低负载时，client get所返回的空包同样会占用不少的带宽。**

------

##### TPS(MySQL transaction)

![Image text](https://github.com/lcybo/canal/raw/master/images/transactions.PNG?raw=true)

```
rate(canal_instance_transactions{destination="example"}[2m])
```

**Canal instance处理transaction的TPS，以TRANSACTION_END事件为基准。**

------

##### TPS(Table row)

![Image text](https://github.com/lcybo/canal/raw/master/images/rows.PNG?raw=true)

```
rate(canal_instance_put_rows{destination="example"}[2m])
```

**对应store put操作的tableRows TPS.**

```
rate(canal_instance_get_rows{destination="example"}[2m])
```

**对应client get操作的tableRows TPS.**

```
rate(canal_instance_ack_rows{destination="example"}[2m])
```

**对应client ack操作的tableRows TPS.**

------

##### Client requests

![Image test](https://github.com/lcybo/canal/raw/master/images/reqs.PNG?raw=true)

```
canal_instance_client_packets{destination="example"}
```

**Netty server处理的client requests，以packetType为label分类统计。**

------

##### Empty packets

![Image text](https://github.com/lcybo/canal/raw/master/images/empty.PNG?raw=true)

```
rate(canal_instance_client_empty_batches{destination="example"}[2m])
```

**client get返回每秒空包量。如果正常traffic下，该值很大，考虑使用connector的timeout机制，节省资源。**

```
rate(canal_instance_client_packets{destination="example", packetType="GET"}[2m])
```

**nonempty, 作为empty rate的参照。**

------

##### Response time

![Image text](https://github.com/lcybo/canal/raw/master/images/latency.PNG?raw=true)

```
canal_instance_client_request_latency_bucket{destination="example"}
```

**Histogram, client请求响应时间统计。关于[histogram](https://prometheus.io/docs/concepts/metric_types/#histogram).**

------

##### Event store占用

![Image text](https://github.com/lcybo/canal/raw/master/images/remain_events.PNG?raw=true)

```
canal_instance_store_produce_seq{destination="example"} - canal_instance_store_consume_seq{destination="example"}
```

**Event store内未ack的events数量，实时性受scrape_interval影响。**

------

##### Event store memory占用(KB, 仅memory mode)

![Image text](https://github.com/lcybo/canal/raw/master/images/remain_mem.PNG?raw=true)

```
(canal_instance_store_produce_mem{destination="example"} - canal_instance_store_consume_mem{destination="example"}) / 1024
```

**Event store内未ack的events所占用内存大小，实时性受scrape_interval影响。**

------

##### Client QPS

![Image text](https://github.com/lcybo/canal/raw/master/images/QPS.PNG?raw=true)

```
rate(canal_instance_client_packets{destination="example",packetType="GET"}[2m])
```

GET类型QPS.

```
rate(canal_instance_client_packets{destination="example",packetType="CLIENTACK"}[2m])
```

CLIENTACK类型QPS.

------

##### 状态信息

![Image text](https://github.com/lcybo/canal/raw/master/images/instance.PNG?raw=true)

```
canal_instance{destination="example"}
canal_instance_parser_mode{destination="example"}
canal_instance_store{destination="example"}
```

